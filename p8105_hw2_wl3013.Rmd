---
title: "p8105_hw2_wl3013"
author: "Wen Li_wl3013"
date: "2025-09-27"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(lubridate)
```

# Question 1
### Step 1: Clean the data in pol-month.csv
```{r}
polmonth_df = 
  read.csv(
    "pols-month.csv",
    na = c("NA",".","")
  ) |> 
  janitor::clean_names() |> 
  separate(mon,into = c("year","month","day",sep = "-")) |>
  mutate(
    month = month.name[as.integer(month)] 
    ) |> 
  mutate(
    president = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem"
    )) |> 
  select(-prez_gop, -prez_dem, -day)

```

### Step 2: Clean the data in snp.csv
```{r}
snp_df = 
  read.csv(
    "snp.csv", na = c("NA",".","")
  ) |> 
  janitor::clean_names() |> 
  mutate(date = mdy(date)) |> 
  separate(date,into = c("year","month","day",sep = "-")) |>
  mutate(
    month = month.name[as.integer(month)] 
    ) |> 
  arrange(year, month)
```

### Step 3: Clean the data in unemployment.csv
```{r}
unemp_df = 
  read.csv(
    "unemployment.csv", na = c("NA",".","")
  ) |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = -year,
    names_to = "month",
    values_to = "unemployment"
  ) |> 
  # Capitalize "month" variable
  mutate(
    month_num = match(str_to_title(month), month.abb),
    month = month.name[month_num]
  ) |> 
  select(-month_num)
```

### Step 4: Merge the datasets
```{r}
# Create a subset for snp, choosing the variables we need for merge
snpsub_df = 
  snp_df |> 
  select(year, month, snp_close = close)

# Change the variable type of "year" from int to chr
unemp_df <- unemp_df |> 
  mutate(year = as.character(year))

merge_df = 
  polmonth_df |> 
  left_join(snpsub_df, by = c("year","month")) |> 
  left_join(unemp_df, by = c("year","month"))
```

Conclusion: 
In snp_df, it contains "year", "month", and "close". In the unemp_df, it contains "year", "month", and "unemployment". The same variables among these data is "year" and "month". 
In the final dataset "merge_df", for the variable "close" from snp_data and "unemployment" from unemp_df, only part of the objects have data value.



# Question 2
### Step 1: Import and clean data from excel
```{r}
# Import "Mr. Trash Wheel" sheet from the excel file and clean the data
mr_df <- read_excel("Trashwheel.xlsx", sheet = "Mr. Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    tag = "Mr" #Record the source of data
    ) |> 
  select(-x15,-x16)

# Import "Professor Trash Wheel" sheet
prof_df <- read_excel("Trashwheel.xlsx", sheet = "Professor Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    year = as.character(year),
    tag = "Prof"
  )

# Import "Captain Trash Wheel" sheet
cap_df <- read_excel("Trashwheel.xlsx", sheet = "Captain Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    year = as.character(year),
    tag = "Cap"
  )

# Import "Gwynnda Trash Wheel" sheet
gwy_df <- read_excel("Trashwheel.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    year = as.character(year),
    tag = "Gwy"
  )

```

### Step 2: Combines these datasets into one
```{r}
trashall_df <- bind_rows(mr_df, prof_df, cap_df, gwy_df) |> 
  arrange(year, month, dumpster)
```

### Step 3: Conclude the dataset
```{r}
# Calculate the statistics for answering the questions
prof_weight <- trashall_df |> 
  filter(tag == "Prof") |> 
  summarise(total_weight = sum(weight_tons, na.rm = TRUE)) |> 
  pull(total_weight)

gw_cigs <- trashall_df |> 
  filter(tag == "Gwy", year == 2022, month == "June") |> 
  summarise(total_weight = sum(cigarette_butts, na.rm = TRUE)) |> 
  pull(total_weight)
```
The final dataset contains `r nrow(trashall_df)` observations, with essentials variables regarding these research. For example, each row represents the contents of dumster load, the trash weight in tons, the number of cigarette butts, the number of sports balls, and the time of the data collected.
For the questions, Professor Trash Wheel has collected `r prof_weight` tons of trash, and Gwynnda collected 18120 cigarette butts in June 2022.


# Question 3
### Step 1: Import data and clean data
```{r}
# Write a function to standardize the name of county
norm_borough <- function(x) {
  x |> 
    str_squish() |> str_to_title() |> 
    recode(
      "New York" = "Manhattan", "New York County" = "Manhattan",
      "Kings" = "Brooklyn", "Kings County" = "Brooklyn",
      "Queens" = "Queens", "Queens County" = "Queens",
      "Bronx" = "Bronx", "Bronx County" = "Bronx",
      "Richmond" = "Staten Island", "Richmond County" = "Staten Island"
    )
}

# Import "zip_zori" file and clean the data
zori_clean_df = 
  read.csv(
    "zip_zori.csv",
    na = c("NA",".","")
  ) |> 
  janitor::clean_names() |> 
  # Change date from rows to a column
  pivot_longer(
    cols = 10:last_col(),
    names_to = "date",
    values_to = "zori"
  ) |> 
  # Clean the variable date
  mutate(
    date = str_remove(date,"^x") |> 
      str_replace_all("_","-")
  ) |> 
  transmute(
    zip_code = region_name,
    borough = norm_borough(county_name),
    date,
    zori
  )

# Import "zip_codes" file and clean the data
codes_clean_df = 
  read.csv(
    "zip_codes.csv",
    na = c("NA",".","")
  ) |> 
  janitor::clean_names() |> 
  mutate(
    date = mdy(file_date),
    date = as.character(date)
  )  |> 
  select(-file_date) |> 
  transmute(
    zip_code,
    borough = norm_borough(county),
    date,
    neighborhood
  ) |> 
  distinct(zip_code, borough, neighborhood, .keep_all =  TRUE)

```

### Step 2: Combine 2 datasets
```{r}
# For codes_clean_df, only keep one row for each zip code, preventing "many-to-many" match
codes_one <- codes_clean_df |> 
  arrange(zip_code, borough, neighborhood) |> 
  distinct(zip_code, .keep_all = TRUE)

# Delect columns with the same name
zori_for_join <- zori_clean_df |> 
  select(-any_of(c("borough", "neighborhood")))

# Join the datasets and sort
final_tidy_df <- zori_for_join |> 
  left_join(
    codes_one |>  select(zip_code, borough, neighborhood),
    by = "zip_code",
    relationship = "many-to-one",
    suffix = c("", "_from_codes")
  ) |> 
  arrange(date, borough, neighborhood, zip_code)

```

### Step 3: Describe the final_tidy_df and answer the questions
```{r}
sum_stats <- list(
  n_rows = nrow(final_tidy_df),
  n_unique_zips = n_distinct(final_tidy_df$zip_code),
  n_unique_neighs = n_distinct(final_tidy_df$neighborhood)
)
```
There is `r nrow(final_tidy_df)` observations in this final tidy data set. There are `r n_distinct(final_tidy_df$zip_code)` unique ZIP codes, and `r n_distinct(final_tidy_df$neighborhood)` unique neighborhoods.


```{r}
zips_not_zori <- codes_clean_df |> 
  anti_join(zori_clean_df |>  distinct(zip_code), by = "zip_code") |> 
  arrange(borough, zip_code)

zips_missing_examples <- zips_not_zori |>  slice_head(n = 10)
```
There are `r nrow(zips_not_zori)` observations appear in the ZIP code data set but not in the Zillow Rental Price data set. 
One of the reason for that is the Zillow data set doesn't cover in these area, or maybe these areas are non-residential zones.


```{r}
# Compare the drop price between 2020.01 and 2021.01
jan_2020 <- final_tidy_df |> 
  filter(date >= ymd("2020-01-01"), date < ymd("2020-02-01"))

jan_2021 <- final_tidy_df |> 
  filter(date >= ymd("2021-01-01"), date < ymd("2021-02-01"))

drop_2021_vs_2020 <- jan_2020 |> 
  select(zip_code, zori_2020_01 = zori) |> 
  inner_join(jan_2021 |>  select(zip_code, zori_2021_01 = zori), by = "zip_code") |> 
  mutate(drop = zori_2021_01 - zori_2020_01) |> 
  arrange(drop) 

# Select top 10 price-drop zip code
top10_drop <- drop_2021_vs_2020 |>  
  slice_head(n = 10)
print(top10_drop)
```
Conclusion: The top 10 price-drop ranging from 684.9 to 912.6, and all 10 areas are Manhattan. The largest price-drop is zip code 10007, with 912.6 price drop comparing 2020 January to 2021 January. This pattern matches the COVID shock, remote work, shutdown of offices, and restaurants hit office-concentric Manhattan hardest.